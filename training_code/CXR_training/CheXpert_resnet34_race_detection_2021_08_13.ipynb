{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Input, Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "#pip install image-classifiers==1.0.0b1\n",
    "from classification_models.tfkeras import Classifiers\n",
    "\n",
    "#CheXpert images can be found: https://stanfordaimi.azurewebsites.net/datasets/8cbd9ed4-2eb9-4565-affc-111cf4f7ebe2\n",
    "\n",
    "#Demographic labels can be found: https://stanfordaimi.azurewebsites.net/datasets/192ada7c-4d43-466e-b8bb-b81992bb80cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2021)\n",
    "python_random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Number of devices: 4\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:51: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = pd.DataFrame(pd.read_excel(\"CHEXPERT DEMO.xlsx\", engine='openpyxl')) #pip install openpyxl\n",
    "data_df = pd.read_csv('chexpert_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATIENT</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE_AT_CXR</th>\n",
       "      <th>PRIMARY_RACE</th>\n",
       "      <th>ETHNICITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PATIENT, GENDER, AGE_AT_CXR, PRIMARY_RACE, ETHNICITY]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>FrontalLateral</th>\n",
       "      <th>Position</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Path, Sex, Age, FrontalLateral, Position, No Finding, Enlarged Cardiomediastinum, Cardiomegaly, Lung Opacity, Lung Lesion, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Pleural Effusion, Pleural Other, Fracture, Support Devices]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 223414\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of images: \" + str(len(data_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients: 65401\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of patients: \" + str(len(demo_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split =  data_df.Path.str.split(\"/\", expand = True)\n",
    "data_df[\"patient_id\"] = split[2]\n",
    "demo_df = demo_df.rename(columns={'PATIENT': 'patient_id'})\n",
    "combine_df = data_df.merge(demo_df, on=\"patient_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "White                                        102402\n",
       "Other                                         28095\n",
       "White, non-Hispanic                           22154\n",
       "Asian                                         20434\n",
       "Unknown                                       15186\n",
       "Black or African American                      9909\n",
       "Race and Ethnicity Unknown                     8716\n",
       "Other, Hispanic                                3621\n",
       "Native Hawaiian or Other Pacific Islander      2809\n",
       "Asian, non-Hispanic                            2793\n",
       "Black, non-Hispanic                            2000\n",
       "White, Hispanic                                 922\n",
       "Other, non-Hispanic                             566\n",
       "American Indian or Alaska Native                457\n",
       "Patient Refused                                 405\n",
       "Pacific Islander, non-Hispanic                  337\n",
       "Native American, non-Hispanic                    55\n",
       "Black, Hispanic                                  52\n",
       "Asian, Hispanic                                  37\n",
       "Native American, Hispanic                        25\n",
       "White or Caucasian                               13\n",
       "Pacific Islander, Hispanic                       10\n",
       "Asian - Historical Conv                           8\n",
       "Name: PRIMARY_RACE, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.PRIMARY_RACE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df.insert(3, \"race\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (combine_df.PRIMARY_RACE.str.contains(\"Black\", na=False))\n",
    "combine_df.loc[mask, \"race\"] = \"BLACK/AFRICAN AMERICAN\"\n",
    "\n",
    "mask = (combine_df.PRIMARY_RACE.str.contains(\"White\", na=False))\n",
    "combine_df.loc[mask, \"race\"] = \"WHITE\"\n",
    "\n",
    "mask = (combine_df.PRIMARY_RACE.str.contains(\"Asian\", na=False))\n",
    "combine_df.loc[mask, \"race\"] = \"ASIAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all labels that are not asian, black or white\n",
    "combine_df = combine_df[combine_df.race.isin(['ASIAN','BLACK/AFRICAN AMERICAN','WHITE'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Non-Hispanic/Non-Latino    149268\n",
       "Unknown                      6546\n",
       "Hispanic/Latino              4726\n",
       "Patient Refused               160\n",
       "Not Hispanic                   15\n",
       "Hispanic                        1\n",
       "Name: ETHNICITY, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.ETHNICITY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only non-hispanic labels\n",
    "combine_df = combine_df[combine_df.ETHNICITY.isin([\"Non-Hispanic/Non-Latino\",\"Not Hispanic\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frontal images only (AP/PA)\n",
    "combine_df = combine_df[combine_df[\"FrontalLateral\"]==\"Frontal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images after inclusion/exclusion criteria: 127130\n"
     ]
    }
   ],
   "source": [
    "print(\"Total images after inclusion/exclusion criteria: \" + str(len(combine_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patients after inclusion/exclusion criteria: 64540\n"
     ]
    }
   ],
   "source": [
    "print(\"Total patients after inclusion/exclusion criteria: \" + str(data_df.patient_id.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = combine_df\n",
    "data_df.insert(5, \"split\",\"none\", True)\n",
    "unique_sub_id = data_df.patient_id.unique()\n",
    "\n",
    "train_percent, valid_percent, test_percent = 0.60, 0.10, 0.30\n",
    "\n",
    "unique_sub_id = shuffle(unique_sub_id)\n",
    "value1 = (round(len(unique_sub_id)*train_percent))\n",
    "value2 = (round(len(unique_sub_id)*valid_percent))\n",
    "value3 = value1 + value2\n",
    "value4 = (round(len(unique_sub_id)*test_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients in training set: 25730\n"
     ]
    }
   ],
   "source": [
    "print(\"Patients in training set: \" + str(value1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients in validation set: 4288\n"
     ]
    }
   ],
   "source": [
    "print(\"Patients in validation set: \" + str(value2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients in testing set: 12865\n"
     ]
    }
   ],
   "source": [
    "print(\"Patients in testing set: \" + str(value4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = shuffle(data_df)\n",
    "\n",
    "train_sub_id = unique_sub_id[:value1]\n",
    "validate_sub_id = unique_sub_id[value1:value3]\n",
    "test_sub_id = unique_sub_id[value3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[data_df.patient_id.isin(train_sub_id), \"split\"]=\"train\"\n",
    "data_df.loc[data_df.patient_id.isin(validate_sub_id), \"split\"]=\"validate\"\n",
    "data_df.loc[data_df.patient_id.isin(test_sub_id), \"split\"]=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train       0.599481\n",
       "test        0.300818\n",
       "validate    0.099701\n",
       "Name: split, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.split.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHITE                     99037\n",
       "ASIAN                     18830\n",
       "BLACK/AFRICAN AMERICAN     9263\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHITE                     0.779021\n",
       "ASIAN                     0.148116\n",
       "BLACK/AFRICAN AMERICAN    0.072862\n",
       "Name: race, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.race.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up\n",
    "data_df = data_df.sort_values(by=['Path'])\n",
    "data_df = data_df.reset_index()\n",
    "data_df = data_df.drop(columns=['index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[data_df.Position.isin(['AP','PA'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data_df[data_df.split==\"train\"]\n",
    "validation_df = data_df[data_df.split==\"validate\"]\n",
    "test_df = data_df[data_df.split==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#False indicates no patient_id shared between groups\n",
    "\n",
    "unique_train_id = train_df.patient_id.unique()\n",
    "unique_validation_id = validation_df.patient_id.unique()\n",
    "unique_test_id = test_df.patient_id.unique()\n",
    "all_id = np.concatenate((unique_train_id, unique_validation_id, unique_test_id), axis=None)\n",
    "\n",
    "def contains_duplicates(X):\n",
    "    return len(np.unique(X)) != len(X)\n",
    "\n",
    "contains_duplicates(all_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT, WIDTH = 320, 320\n",
    "\n",
    "arc_name = \"CHEXPERT-\" + str(HEIGHT) + \"x\" + str(WIDTH) + \"_60-10-30-split-resnet34-Float16_3-race_detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34, preprocess_input = Classifiers.get('resnet34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    input_a = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    base_model = resnet34(input_tensor=input_a, include_top=False, input_shape=(HEIGHT,WIDTH,3), weights='imagenet')\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(3, name='dense_logits')(x)\n",
    "    output = Activation('softmax', dtype='float32', name='predictions')(x)\n",
    "    model = Model(inputs=[input_a], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "momentum_val=0.9\n",
    "decay_val= 0.0\n",
    "train_batch_size = 256 # may need to reduce batch size if OOM error occurs\n",
    "test_batch_size = 256\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.1, patience=2, min_lr=1e-5, verbose=1)\n",
    "\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, decay=decay_val)\n",
    "adam_opt = tf.keras.mixed_precision.LossScaleOptimizer(adam_opt)\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    model.compile(optimizer=adam_opt,\n",
    "                    loss=tf.losses.CategoricalCrossentropy(),\n",
    "                    metrics=[\n",
    "                        tf.keras.metrics.AUC(curve='ROC', name='ROC-AUC'),\n",
    "                        tf.keras.metrics.AUC(curve='PR', name='PR-AUC')\n",
    "                    ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "            rotation_range=15,\n",
    "            fill_mode='constant',\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.1,\n",
    "            preprocessing_function=preprocess_input\n",
    "            )\n",
    "\n",
    "validate_gen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76205 validated image filenames belonging to 3 classes.\n",
      "Found 12673 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = train_gen.flow_from_dataframe(train_df, directory=\"/tf/notebooks/SSD_data/chexpert_directory/\", x_col=\"Path\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=True,seed=2021,batch_size=train_batch_size, dtype='float32')\n",
    "validate_batches = validate_gen.flow_from_dataframe(validation_df, directory=\"/tf/notebooks/SSD_data/chexpert_directory/\", x_col=\"Path\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=False,batch_size=test_batch_size, dtype='float32')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = math.ceil(len(train_df) / train_batch_size)\n",
    "val_epoch = math.ceil(len(validation_df) / test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_date = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ES = EarlyStopping(monitor='val_loss', mode='min', patience=4, restore_best_weights=True)\n",
    "checkloss = ModelCheckpoint(\"../saved_models/racial_bias/trials/\" + str(arc_name) + \"_CXR_LR-\" + str(learning_rate) + \"_\" + var_date+\"_epoch:{epoch:03d}_val_loss:{val_loss:.5f}.hdf5\", monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 108 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 108 all-reduces with algorithm = nccl, num_packs = 1\n",
      "298/298 [==============================] - 693s 2s/step - loss: 0.5046 - ROC-AUC: 0.9309 - PR-AUC: 0.8793 - val_loss: 0.7998 - val_ROC-AUC: 0.8627 - val_PR-AUC: 0.7413\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.79985, saving model to ../saved_models/racial_bias/trials/CHEXPERT-320x320_60-10-30-split-resnet34-Float16_3-race_detection_CXR_LR-0.001_20210813-141741_epoch:001_val_loss:0.79985.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "298/298 [==============================] - 596s 2s/step - loss: 0.3432 - ROC-AUC: 0.9673 - PR-AUC: 0.9412 - val_loss: 1.1863 - val_ROC-AUC: 0.7341 - val_PR-AUC: 0.5628\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.79985\n",
      "Epoch 3/100\n",
      "298/298 [==============================] - 549s 2s/step - loss: 0.2907 - ROC-AUC: 0.9760 - PR-AUC: 0.9564 - val_loss: 0.4572 - val_ROC-AUC: 0.9503 - val_PR-AUC: 0.9109\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.79985 to 0.45717, saving model to ../saved_models/racial_bias/trials/CHEXPERT-320x320_60-10-30-split-resnet34-Float16_3-race_detection_CXR_LR-0.001_20210813-141741_epoch:003_val_loss:0.45717.hdf5\n",
      "Epoch 4/100\n",
      "298/298 [==============================] - 580s 2s/step - loss: 0.2646 - ROC-AUC: 0.9797 - PR-AUC: 0.9628 - val_loss: 0.2939 - val_ROC-AUC: 0.9760 - val_PR-AUC: 0.9578\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.45717 to 0.29387, saving model to ../saved_models/racial_bias/trials/CHEXPERT-320x320_60-10-30-split-resnet34-Float16_3-race_detection_CXR_LR-0.001_20210813-141741_epoch:004_val_loss:0.29387.hdf5\n",
      "Epoch 5/100\n",
      "298/298 [==============================] - 525s 2s/step - loss: 0.2446 - ROC-AUC: 0.9826 - PR-AUC: 0.9680 - val_loss: 0.3089 - val_ROC-AUC: 0.9751 - val_PR-AUC: 0.9554\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29387\n",
      "Epoch 6/100\n",
      "298/298 [==============================] - 530s 2s/step - loss: 0.2296 - ROC-AUC: 0.9843 - PR-AUC: 0.9711 - val_loss: 0.2988 - val_ROC-AUC: 0.9776 - val_PR-AUC: 0.9600\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.29387\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/100\n",
      "298/298 [==============================] - 522s 2s/step - loss: 0.1768 - ROC-AUC: 0.9900 - PR-AUC: 0.9813 - val_loss: 0.2245 - val_ROC-AUC: 0.9841 - val_PR-AUC: 0.9715\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.29387 to 0.22449, saving model to ../saved_models/racial_bias/trials/CHEXPERT-320x320_60-10-30-split-resnet34-Float16_3-race_detection_CXR_LR-0.001_20210813-141741_epoch:007_val_loss:0.22449.hdf5\n",
      "Epoch 8/100\n",
      "298/298 [==============================] - 523s 2s/step - loss: 0.1608 - ROC-AUC: 0.9915 - PR-AUC: 0.9840 - val_loss: 0.2274 - val_ROC-AUC: 0.9847 - val_PR-AUC: 0.9723\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.22449\n",
      "Epoch 9/100\n",
      "298/298 [==============================] - 529s 2s/step - loss: 0.1544 - ROC-AUC: 0.9922 - PR-AUC: 0.9852 - val_loss: 0.2441 - val_ROC-AUC: 0.9829 - val_PR-AUC: 0.9693\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22449\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 10/100\n",
      "298/298 [==============================] - 529s 2s/step - loss: 0.1449 - ROC-AUC: 0.9929 - PR-AUC: 0.9866 - val_loss: 0.2254 - val_ROC-AUC: 0.9848 - val_PR-AUC: 0.9728\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.22449\n",
      "Epoch 11/100\n",
      "298/298 [==============================] - 529s 2s/step - loss: 0.1411 - ROC-AUC: 0.9931 - PR-AUC: 0.9871 - val_loss: 0.2229 - val_ROC-AUC: 0.9850 - val_PR-AUC: 0.9727\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.22449 to 0.22293, saving model to ../saved_models/racial_bias/trials/CHEXPERT-320x320_60-10-30-split-resnet34-Float16_3-race_detection_CXR_LR-0.001_20210813-141741_epoch:011_val_loss:0.22293.hdf5\n",
      "Epoch 12/100\n",
      "298/298 [==============================] - 514s 2s/step - loss: 0.1407 - ROC-AUC: 0.9932 - PR-AUC: 0.9873 - val_loss: 0.2305 - val_ROC-AUC: 0.9845 - val_PR-AUC: 0.9722\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22293\n",
      "Epoch 13/100\n",
      "298/298 [==============================] - 509s 2s/step - loss: 0.1400 - ROC-AUC: 0.9933 - PR-AUC: 0.9873 - val_loss: 0.2234 - val_ROC-AUC: 0.9851 - val_PR-AUC: 0.9731\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.22293\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 14/100\n",
      "298/298 [==============================] - 519s 2s/step - loss: 0.1406 - ROC-AUC: 0.9932 - PR-AUC: 0.9871 - val_loss: 0.2277 - val_ROC-AUC: 0.9848 - val_PR-AUC: 0.9728\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.22293\n",
      "Epoch 15/100\n",
      "298/298 [==============================] - 517s 2s/step - loss: 0.1379 - ROC-AUC: 0.9935 - PR-AUC: 0.9878 - val_loss: 0.2260 - val_ROC-AUC: 0.9850 - val_PR-AUC: 0.9729\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.22293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe2de88e898>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_batches,\n",
    "            validation_data=validate_batches,\n",
    "            epochs=100,\n",
    "            steps_per_epoch=int(train_epoch),\n",
    "            validation_steps=int(val_epoch),\n",
    "            workers=32,\n",
    "            max_queue_size=50,\n",
    "            shuffle=False,\n",
    "            callbacks=[checkloss, reduce_lr, ES]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38240 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = validate_gen.flow_from_dataframe(test_df, directory=\"/tf/notebooks/fishtank/radiology_datasets/CheXpert_Xray_dataset/resize_chexpert_320x320/chexpert_data/\", x_col=\"Path\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=False,batch_size=test_batch_size, dtype='float32')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 49s 316ms/step\n"
     ]
    }
   ],
   "source": [
    "with mirrored_strategy.scope():\n",
    "\n",
    "    multilabel_predict_test = model.predict(test_batches, max_queue_size=10, verbose=1, steps=math.ceil(len(test_df)/test_batch_size), workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prediction = multilabel_predict_test\n",
    "input_df = test_df\n",
    "input_prediction_df = pd.DataFrame(input_prediction)\n",
    "true_logits = pd.DataFrame()\n",
    "loss_log = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_calc(input_prediction_df, input_df):\n",
    "    ground_truth = input_df.race\n",
    "    pathology_array=[\n",
    "        'ASIAN',\n",
    "        'BLACK/AFRICAN AMERICAN',\n",
    "        'WHITE'\n",
    "        ]\n",
    "    i=0\n",
    "    auc_array = []\n",
    "    for pathology in pathology_array:\n",
    "    \n",
    "        new_truth = (ground_truth.str.contains(pathology)).apply(int)\n",
    "        input_prediction_val = input_prediction_df[i]\n",
    "        val = input_prediction_val\n",
    "        AUC = roc_auc_score(new_truth, val)\n",
    "        true_logits.insert(i, i, new_truth, True)\n",
    "        auc_array.append(AUC)\n",
    "        i += 1\n",
    "        \n",
    "    progress_df = pd.DataFrame({'Study':pathology_array, 'AUC':auc_array})\n",
    "    print(progress_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Study       AUC\n",
      "0                   ASIAN  0.974955\n",
      "1  BLACK/AFRICAN AMERICAN  0.982292\n",
      "2                   WHITE  0.974108\n"
     ]
    }
   ],
   "source": [
    "stat_calc(input_prediction_df, input_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
