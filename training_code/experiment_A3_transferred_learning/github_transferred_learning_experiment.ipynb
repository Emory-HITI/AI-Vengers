{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model was made using a docker image\n",
    "# Docker image can be found at https://hub.docker.com/r/blackboxradiology/tf-2.6_with_pytorch\n",
    "# docker pull blackboxradiology/tf-2.6_with_pytorch\n",
    "\n",
    "# python version 3.6.9\n",
    "# mayplotlib version 3.3.4\n",
    "# numpy version 1.19.5\n",
    "# pandas version 1.1.5\n",
    "# PIL version 8.2.0\n",
    "# sklearn version 0.24.2\n",
    "# tensorflow version 2.6.0\n",
    "\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random as python_random\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import auc, accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Input, Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/mixed_precision/loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
     ]
    }
   ],
   "source": [
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "# All preprocessing steps of MIMIC .jpg images are included in this repository\n",
    "# Image data preprocessing include resizing to 320x320\n",
    "# and normalizing images with ImageNet mean and standard deviation values using\n",
    "# from tensorflow.keras.applications.densenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>PerformedProcedureStepDescription</th>\n",
       "      <th>ViewPosition</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Columns</th>\n",
       "      <th>StudyDate</th>\n",
       "      <th>StudyTime</th>\n",
       "      <th>ProcedureCodeSequence_CodeMeaning</th>\n",
       "      <th>ViewCodeSequence_CodeMeaning</th>\n",
       "      <th>PatientOrientationCodeSequence_CodeMeaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dicom_id, subject_id, study_id, PerformedProcedureStepDescription, ViewPosition, Rows, Columns, StudyDate, StudyTime, ProcedureCodeSequence_CodeMeaning, ViewCodeSequence_CodeMeaning, PatientOrientationCodeSequence_CodeMeaning]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata_df is mimic-cxr-2.0.0-metadata.csv from https://physionet.org/content/mimic-cxr-jpg/2.0.0/\n",
    "metadata_df = pd.read_csv('mimic-cxr-2.0.0-metadata.csv')\n",
    "metadata_df[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>admission_location</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>insurance</th>\n",
       "      <th>language</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>edregtime</th>\n",
       "      <th>edouttime</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subject_id, hadm_id, admittime, dischtime, deathtime, admission_type, admission_location, discharge_location, insurance, language, marital_status, ethnicity, edregtime, edouttime, hospital_expire_flag]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demographic_df is the addmissions.csv from the \"core\" directory found at https://physionet.org/content/mimiciv/1.0/\n",
    "demographic_df = pd.read_csv('admissions.csv')\n",
    "demographic_df[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subject_id, study_id, Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Fracture, Lung Lesion, Lung Opacity, No Finding, Pleural Effusion, Pleural Other, Pneumonia, Pneumothorax, Support Devices]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mimic-cxr-2.0.0-chexpert.csv can be found at https://physionet.org/content/mimic-cxr-jpg/2.0.0/\n",
    "pathology_df = pd.read_csv('mimic-cxr-2.0.0-chexpert.csv')\n",
    "pathology_df = pathology_df.fillna(0)\n",
    "pathology_df[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 377110\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of images: \" + str(len(metadata_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients: 65379\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of patients: \" + str(metadata_df.subject_id.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHITE                            337630\n",
       "BLACK/AFRICAN AMERICAN            80293\n",
       "HISPANIC/LATINO                   29823\n",
       "OTHER                             26813\n",
       "ASIAN                             24506\n",
       "UNKNOWN                           19400\n",
       "UNABLE TO OBTAIN                   3740\n",
       "AMERICAN INDIAN/ALASKA NATIVE      1535\n",
       "Name: ethnicity, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_df.ethnicity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER_WHITE                                                   2489\n",
       "UNKNOWN_WHITE                                                 1131\n",
       "BLACK/AFRICAN AMERICAN_OTHER                                   560\n",
       "UNABLE TO OBTAIN_WHITE                                         308\n",
       "HISPANIC/LATINO_OTHER                                          307\n",
       "HISPANIC/LATINO_WHITE                                          204\n",
       "HISPANIC/LATINO_UNKNOWN                                        173\n",
       "BLACK/AFRICAN AMERICAN_WHITE                                   168\n",
       "BLACK/AFRICAN AMERICAN_UNKNOWN                                 156\n",
       "OTHER_UNKNOWN                                                  130\n",
       "BLACK/AFRICAN AMERICAN_HISPANIC/LATINO                         111\n",
       "ASIAN_OTHER                                                     98\n",
       "UNABLE TO OBTAIN_UNKNOWN                                        49\n",
       "ASIAN_WHITE                                                     46\n",
       "BLACK/AFRICAN AMERICAN_UNABLE TO OBTAIN                         44\n",
       "ASIAN_UNKNOWN                                                   41\n",
       "OTHER_UNABLE TO OBTAIN                                          29\n",
       "AMERICAN INDIAN/ALASKA NATIVE_WHITE                             28\n",
       "HISPANIC/LATINO_UNABLE TO OBTAIN                                21\n",
       "OTHER_UNKNOWN_WHITE                                             21\n",
       "AMERICAN INDIAN/ALASKA NATIVE_OTHER                             19\n",
       "OTHER_UNABLE TO OBTAIN_WHITE                                    19\n",
       "HISPANIC/LATINO_OTHER_WHITE                                     19\n",
       "ASIAN_UNABLE TO OBTAIN                                          18\n",
       "HISPANIC/LATINO_OTHER_UNKNOWN                                   16\n",
       "AMERICAN INDIAN/ALASKA NATIVE_BLACK/AFRICAN AMERICAN            11\n",
       "BLACK/AFRICAN AMERICAN_OTHER_WHITE                              11\n",
       "ASIAN_BLACK/AFRICAN AMERICAN                                    11\n",
       "UNABLE TO OBTAIN_UNKNOWN_WHITE                                   9\n",
       "HISPANIC/LATINO_UNKNOWN_WHITE                                    8\n",
       "AMERICAN INDIAN/ALASKA NATIVE_UNABLE TO OBTAIN                   8\n",
       "BLACK/AFRICAN AMERICAN_HISPANIC/LATINO_OTHER                     8\n",
       "ASIAN_HISPANIC/LATINO                                            7\n",
       "AMERICAN INDIAN/ALASKA NATIVE_HISPANIC/LATINO                    6\n",
       "AMERICAN INDIAN/ALASKA NATIVE_UNKNOWN                            5\n",
       "BLACK/AFRICAN AMERICAN_OTHER_UNKNOWN                             4\n",
       "AMERICAN INDIAN/ALASKA NATIVE_ASIAN                              4\n",
       "AMERICAN INDIAN/ALASKA NATIVE_BLACK/AFRICAN AMERICAN_OTHER       4\n",
       "ASIAN_UNABLE TO OBTAIN_UNKNOWN                                   3\n",
       "ASIAN_UNKNOWN_WHITE                                              3\n",
       "HISPANIC/LATINO_UNABLE TO OBTAIN_UNKNOWN                         3\n",
       "BLACK/AFRICAN AMERICAN_UNKNOWN_WHITE                             2\n",
       "BLACK/AFRICAN AMERICAN_UNABLE TO OBTAIN_UNKNOWN                  2\n",
       "BLACK/AFRICAN AMERICAN_HISPANIC/LATINO_WHITE                     2\n",
       "AMERICAN INDIAN/ALASKA NATIVE_OTHER_WHITE                        2\n",
       "OTHER_UNABLE TO OBTAIN_UNKNOWN_WHITE                             1\n",
       "ASIAN_HISPANIC/LATINO_OTHER                                      1\n",
       "ASIAN_BLACK/AFRICAN AMERICAN_HISPANIC/LATINO_OTHER               1\n",
       "ASIAN_BLACK/AFRICAN AMERICAN_OTHER                               1\n",
       "ASIAN_OTHER_WHITE                                                1\n",
       "AMERICAN INDIAN/ALASKA NATIVE_ASIAN_OTHER_WHITE                  1\n",
       "ASIAN_OTHER_UNKNOWN                                              1\n",
       "ASIAN_HISPANIC/LATINO_WHITE                                      1\n",
       "ASIAN_BLACK/AFRICAN AMERICAN_UNKNOWN                             1\n",
       "BLACK/AFRICAN AMERICAN_OTHER_UNKNOWN_WHITE                       1\n",
       "ASIAN_BLACK/AFRICAN AMERICAN_WHITE                               1\n",
       "BLACK/AFRICAN AMERICAN_OTHER_UNABLE TO OBTAIN                    1\n",
       "AMERICAN INDIAN/ALASKA NATIVE_UNKNOWN_WHITE                      1\n",
       "BLACK/AFRICAN AMERICAN_HISPANIC/LATINO_OTHER_WHITE               1\n",
       "BLACK/AFRICAN AMERICAN_HISPANIC/LATINO_OTHER_UNKNOWN             1\n",
       "Name: ethnicity, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove patients who have inconsistent documented race information\n",
    "# credit to github.com/robintibor\n",
    "ethnicity_df = demographic_df.loc[:,['subject_id', 'ethnicity']].drop_duplicates()\n",
    "\n",
    "v = ethnicity_df.subject_id.value_counts()\n",
    "subject_id_more_than_once = v.index[v.gt(1)]\n",
    "\n",
    "ambiguous_ethnicity_df = ethnicity_df[ethnicity_df.subject_id.isin(subject_id_more_than_once)]\n",
    "inconsistent_race = ambiguous_ethnicity_df.subject_id.unique()\n",
    "\n",
    "grouped = ambiguous_ethnicity_df.groupby('subject_id')\n",
    "grouped.aggregate(lambda x: \"_\".join(sorted(x))).ethnicity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.merge(metadata_df,ethnicity_df,on='subject_id')\n",
    "merge_df = merge_df[~merge_df.subject_id.isin(inconsistent_race)]\n",
    "merge_df = merge_df.rename(columns={\"ethnicity\": \"race\"})\n",
    "merge_df = merge_df[merge_df.race.isin(['ASIAN','BLACK/AFRICAN AMERICAN','WHITE'])]\n",
    "merge_df = merge_df[merge_df.ViewPosition.isin(['AP','PA'])]\n",
    "merge_df = pd.merge(merge_df,pathology_df[['study_id','Atelectasis','Cardiomegaly','Consolidation','Edema','Pleural Effusion','Enlarged Cardiomediastinum','Fracture','Lung Lesion','Lung Opacity','No Finding','Pleural Other','Pneumonia','Pneumothorax','Support Devices']],on='study_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images after inclusion/exclusion criteria: 183217\n"
     ]
    }
   ],
   "source": [
    "print(\"Total images after inclusion/exclusion criteria: \" + str(len(merge_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patients after inclusion/exclusion criteria: 43209\n"
     ]
    }
   ],
   "source": [
    "print(\"Total patients after inclusion/exclusion criteria: \" + str(merge_df.subject_id.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = merge_df\n",
    "data_df.insert(5, \"split\",\"none\", True)\n",
    "unique_sub_id = data_df.subject_id.unique()\n",
    "\n",
    "train_percent, valid_percent, test_percent = 0.80, 0.10, 0.10\n",
    "\n",
    "unique_sub_id = shuffle(unique_sub_id)\n",
    "value1 = (round(len(unique_sub_id)*train_percent))\n",
    "value2 = (round(len(unique_sub_id)*valid_percent))\n",
    "value3 = value1 + value2\n",
    "value4 = (round(len(unique_sub_id)*test_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients in training set: 34567\n"
     ]
    }
   ],
   "source": [
    "print(\"Patients in training set: \" + str(value1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients in validation set: 4321\n"
     ]
    }
   ],
   "source": [
    "print(\"Patients in validation set: \" + str(value2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients in testing set: 4321\n"
     ]
    }
   ],
   "source": [
    "print(\"Patients in testing set: \" + str(value4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = shuffle(data_df)\n",
    "\n",
    "train_sub_id = unique_sub_id[:value1]\n",
    "validate_sub_id = unique_sub_id[value1:value3]\n",
    "test_sub_id = unique_sub_id[value3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[data_df.subject_id.isin(train_sub_id), \"split\"]=\"train\"\n",
    "data_df.loc[data_df.subject_id.isin(validate_sub_id), \"split\"]=\"validate\"\n",
    "data_df.loc[data_df.subject_id.isin(test_sub_id), \"split\"]=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train       0.795952\n",
       "validate    0.104057\n",
       "test        0.099991\n",
       "Name: split, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.split.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHITE                     141873\n",
       "BLACK/AFRICAN AMERICAN     34238\n",
       "ASIAN                       7106\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHITE                     0.774344\n",
       "BLACK/AFRICAN AMERICAN    0.186871\n",
       "ASIAN                     0.038785\n",
       "Name: race, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.race.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.subject_id = data_df.subject_id.astype(str)\n",
    "data_df.study_id = data_df.study_id.astype(str)\n",
    "data_df.insert(2, \"path\", \"\")\n",
    "data_df.path = data_df.subject_id.str[0:2]\n",
    "data_df.path = \"p\" + data_df.path\n",
    "data_df.path = data_df.path + \"/p\" + data_df.subject_id + \"/s\" + data_df.study_id + \"/\" + data_df.dicom_id + \".jpg\"\n",
    "data_df = data_df.rename(columns={\"Pleural Effusion\": \"Effusion\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathology_dict=[\n",
    "'Atelectasis',\n",
    "'Cardiomegaly',\n",
    "'Consolidation',\n",
    "'Edema',\n",
    "'Effusion',\n",
    "'Enlarged Cardiomediastinum',\n",
    "'Fracture',\n",
    "'Lung Lesion',\n",
    "'Lung Opacity',\n",
    "'No Finding',\n",
    "'Pleural Other',\n",
    "'Pneumonia',\n",
    "'Pneumothorax',\n",
    "'Support Devices',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atelectasis\n",
      "Cardiomegaly\n",
      "Consolidation\n",
      "Edema\n",
      "Effusion\n",
      "Enlarged Cardiomediastinum\n",
      "Fracture\n",
      "Lung Lesion\n",
      "Lung Opacity\n",
      "No Finding\n",
      "Pleural Other\n",
      "Pneumonia\n",
      "Pneumothorax\n",
      "Support Devices\n"
     ]
    }
   ],
   "source": [
    "#all NaN and -1.0 are mapped to 0.0\n",
    "\n",
    "data_df = data_df.fillna(0)\n",
    "\n",
    "for pathology in pathology_dict:\n",
    "    print(pathology)\n",
    "    data_df[pathology] = data_df[pathology].mask(data_df[pathology]<0.0, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data_df[data_df.split==\"train\"]\n",
    "validation_df = data_df[data_df.split==\"validate\"]\n",
    "test_df = data_df[data_df.split==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#False indicates no patient_id shared between groups\n",
    "\n",
    "unique_train_id = train_df.subject_id.unique()\n",
    "unique_validation_id = validation_df.subject_id.unique()\n",
    "unique_test_id = test_df.subject_id.unique()\n",
    "all_id = np.concatenate((unique_train_id, unique_validation_id, unique_test_id), axis=None)\n",
    "\n",
    "def contains_duplicates(X):\n",
    "    return len(np.unique(X)) != len(X)\n",
    "\n",
    "contains_duplicates(all_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT, WIDTH = 320, 320\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "arc_name = \"MIMIC-\" + str(HEIGHT) + \"x\" + str(WIDTH) + \"_80-10-10-split-DenseNet121-Float16_pathology_detection_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "base_model = DenseNet121(input_tensor=input_a, include_top=False, input_shape=(HEIGHT,WIDTH,3), weights='imagenet')\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(14, name='dense_logits')(x)\n",
    "output = Activation('sigmoid', dtype='float32', name='predictions')(x)\n",
    "model = Model(inputs=[input_a], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "momentum_val=0.9\n",
    "decay_val= 0.0\n",
    "batch_s = 128 # may need to reduce batch size if OOM error occurs\n",
    "train_batch_size = batch_s\n",
    "test_batch_size = 128\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.1, patience=2, min_lr=1e-5, verbose=1)\n",
    "\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, decay=decay_val)\n",
    "adam_opt = tf.keras.mixed_precision.LossScaleOptimizer(adam_opt)\n",
    "\n",
    "\n",
    "model.compile(optimizer=adam_opt,\n",
    "                loss=tf.losses.BinaryCrossentropy(),\n",
    "                metrics=[\n",
    "                    tf.keras.metrics.AUC(curve='ROC', name='ROC-AUC'),\n",
    "                    tf.keras.metrics.AUC(curve='PR', name='PR-AUC')\n",
    "                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "            rotation_range=15,\n",
    "            fill_mode='constant',\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.1,\n",
    "            preprocessing_function=preprocess_input\n",
    "            )\n",
    "\n",
    "validate_gen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#up sampling 'ASIAN' and 'BLACK/AFRICAN AMERICAN' classes\n",
    "train_df = data_df[data_df.split==\"train\"]\n",
    "other_df = train_df[train_df.race!=\"WHITE\"]\n",
    "train_df = pd.concat([other_df, train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 178330 validated image filenames.\n",
      "Found 19065 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_batches = train_gen.flow_from_dataframe(train_df, directory=\"/path/to/directory/\", x_col=\"path\", y_col=pathology_dict, class_mode=\"raw\",target_size=(HEIGHT, WIDTH),shuffle=True,seed=2021,batch_size=train_batch_size, dtype='float32')\n",
    "validate_batches = validate_gen.flow_from_dataframe(validation_df, directory=\"/path/to/directory/\", x_col=\"path\", y_col=pathology_dict, class_mode=\"raw\",target_size=(HEIGHT, WIDTH),shuffle=False,batch_size=test_batch_size, dtype='float32')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = math.ceil(len(train_df) / train_batch_size)\n",
    "val_epoch = math.ceil(len(validation_df) / test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_date = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ES = EarlyStopping(monitor='val_loss', mode='min', patience=4, restore_best_weights=True)\n",
    "checkloss = ModelCheckpoint(\"../saved_models/racial_bias/trials/\" + str(arc_name) + \"_CXR_LR-\" + str(learning_rate) + \"_\" + var_date+\"_epoch:{epoch:03d}_val_loss:{val_loss:.5f}.hdf5\", monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 364 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 364 all-reduces with algorithm = nccl, num_packs = 1\n",
      "1394/1394 [==============================] - 620s 347ms/step - loss: 0.2775 - ROC-AUC: 0.8628 - PR-AUC: 0.5314 - val_loss: 0.2856 - val_ROC-AUC: 0.8577 - val_PR-AUC: 0.5321\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28563, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.001_20211013-165756_epoch:001_val_loss:0.28563.hdf5\n",
      "Epoch 2/100\n",
      "1394/1394 [==============================] - 478s 341ms/step - loss: 0.2646 - ROC-AUC: 0.8777 - PR-AUC: 0.5711 - val_loss: 0.2870 - val_ROC-AUC: 0.8596 - val_PR-AUC: 0.5194\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.28563\n",
      "Epoch 3/100\n",
      "1394/1394 [==============================] - 480s 343ms/step - loss: 0.2596 - ROC-AUC: 0.8834 - PR-AUC: 0.5849 - val_loss: 0.2832 - val_ROC-AUC: 0.8695 - val_PR-AUC: 0.5419\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.28563 to 0.28317, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.001_20211013-165756_epoch:003_val_loss:0.28317.hdf5\n",
      "Epoch 4/100\n",
      "1394/1394 [==============================] - 479s 342ms/step - loss: 0.2562 - ROC-AUC: 0.8871 - PR-AUC: 0.5945 - val_loss: 0.2851 - val_ROC-AUC: 0.8617 - val_PR-AUC: 0.5427\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.28317\n",
      "Epoch 5/100\n",
      "1394/1394 [==============================] - 479s 342ms/step - loss: 0.2532 - ROC-AUC: 0.8903 - PR-AUC: 0.6026 - val_loss: 0.2799 - val_ROC-AUC: 0.8747 - val_PR-AUC: 0.5743\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.28317 to 0.27994, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.001_20211013-165756_epoch:005_val_loss:0.27994.hdf5\n",
      "Epoch 6/100\n",
      "1394/1394 [==============================] - 482s 342ms/step - loss: 0.2508 - ROC-AUC: 0.8928 - PR-AUC: 0.6088 - val_loss: 0.2765 - val_ROC-AUC: 0.8696 - val_PR-AUC: 0.5690\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27994 to 0.27647, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.001_20211013-165756_epoch:006_val_loss:0.27647.hdf5\n",
      "Epoch 7/100\n",
      "1394/1394 [==============================] - 479s 342ms/step - loss: 0.2486 - ROC-AUC: 0.8950 - PR-AUC: 0.6149 - val_loss: 0.2799 - val_ROC-AUC: 0.8693 - val_PR-AUC: 0.5450\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.27647\n",
      "Epoch 8/100\n",
      "1394/1394 [==============================] - 479s 342ms/step - loss: 0.2466 - ROC-AUC: 0.8972 - PR-AUC: 0.6198 - val_loss: 0.2785 - val_ROC-AUC: 0.8752 - val_PR-AUC: 0.5734\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.27647\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/100\n",
      "1394/1394 [==============================] - 479s 342ms/step - loss: 0.2379 - ROC-AUC: 0.9057 - PR-AUC: 0.6431 - val_loss: 0.2566 - val_ROC-AUC: 0.8920 - val_PR-AUC: 0.6133\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.27647 to 0.25662, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.001_20211013-165756_epoch:009_val_loss:0.25662.hdf5\n",
      "Epoch 10/100\n",
      "1394/1394 [==============================] - 479s 342ms/step - loss: 0.2351 - ROC-AUC: 0.9084 - PR-AUC: 0.6501 - val_loss: 0.2565 - val_ROC-AUC: 0.8918 - val_PR-AUC: 0.6137\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.25662 to 0.25653, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.001_20211013-165756_epoch:010_val_loss:0.25653.hdf5\n",
      "Epoch 11/100\n",
      "1394/1394 [==============================] - 482s 344ms/step - loss: 0.2336 - ROC-AUC: 0.9097 - PR-AUC: 0.6541 - val_loss: 0.2572 - val_ROC-AUC: 0.8916 - val_PR-AUC: 0.6126\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.25653\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 12/100\n",
      "1394/1394 [==============================] - 489s 349ms/step - loss: 0.2317 - ROC-AUC: 0.9115 - PR-AUC: 0.6590 - val_loss: 0.2572 - val_ROC-AUC: 0.8914 - val_PR-AUC: 0.6120\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25653\n",
      "Epoch 13/100\n",
      "1394/1394 [==============================] - 480s 343ms/step - loss: 0.2315 - ROC-AUC: 0.9118 - PR-AUC: 0.6596 - val_loss: 0.2567 - val_ROC-AUC: 0.8918 - val_PR-AUC: 0.6132\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.25653\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 14/100\n",
      "1394/1394 [==============================] - 479s 342ms/step - loss: 0.2311 - ROC-AUC: 0.9120 - PR-AUC: 0.6608 - val_loss: 0.2569 - val_ROC-AUC: 0.8917 - val_PR-AUC: 0.6129\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.25653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5bd0ecf390>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_batches,\n",
    "            validation_data=validate_batches,\n",
    "            epochs=100,\n",
    "            steps_per_epoch=int(train_epoch),\n",
    "            validation_steps=int(val_epoch),\n",
    "            workers=32,\n",
    "            max_queue_size=50,\n",
    "            shuffle=True,\n",
    "            callbacks=[checkloss, reduce_lr, ES]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GlobalAveragePooling2D()(model_transfer.layers[-4].output)\n",
    "x = tf.keras.layers.Dense(3, name='dense_logits')(x)\n",
    "output = tf.keras.layers.Activation('softmax', dtype='float32', name='predictions')(x)\n",
    "model = Model(inputs=[model_transfer.input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in model.layers[:-2]:\n",
    "    layers.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 : False\n",
      "random_flip : False\n",
      "random_rotation : False\n",
      "zero_padding2d : False\n",
      "conv1/conv : False\n",
      "conv1/bn : False\n",
      "conv1/relu : False\n",
      "zero_padding2d_1 : False\n",
      "pool1 : False\n",
      "conv2_block1_0_bn : False\n",
      "conv2_block1_0_relu : False\n",
      "conv2_block1_1_conv : False\n",
      "conv2_block1_1_bn : False\n",
      "conv2_block1_1_relu : False\n",
      "conv2_block1_2_conv : False\n",
      "conv2_block1_concat : False\n",
      "conv2_block2_0_bn : False\n",
      "conv2_block2_0_relu : False\n",
      "conv2_block2_1_conv : False\n",
      "conv2_block2_1_bn : False\n",
      "conv2_block2_1_relu : False\n",
      "conv2_block2_2_conv : False\n",
      "conv2_block2_concat : False\n",
      "conv2_block3_0_bn : False\n",
      "conv2_block3_0_relu : False\n",
      "conv2_block3_1_conv : False\n",
      "conv2_block3_1_bn : False\n",
      "conv2_block3_1_relu : False\n",
      "conv2_block3_2_conv : False\n",
      "conv2_block3_concat : False\n",
      "conv2_block4_0_bn : False\n",
      "conv2_block4_0_relu : False\n",
      "conv2_block4_1_conv : False\n",
      "conv2_block4_1_bn : False\n",
      "conv2_block4_1_relu : False\n",
      "conv2_block4_2_conv : False\n",
      "conv2_block4_concat : False\n",
      "conv2_block5_0_bn : False\n",
      "conv2_block5_0_relu : False\n",
      "conv2_block5_1_conv : False\n",
      "conv2_block5_1_bn : False\n",
      "conv2_block5_1_relu : False\n",
      "conv2_block5_2_conv : False\n",
      "conv2_block5_concat : False\n",
      "conv2_block6_0_bn : False\n",
      "conv2_block6_0_relu : False\n",
      "conv2_block6_1_conv : False\n",
      "conv2_block6_1_bn : False\n",
      "conv2_block6_1_relu : False\n",
      "conv2_block6_2_conv : False\n",
      "conv2_block6_concat : False\n",
      "pool2_bn : False\n",
      "pool2_relu : False\n",
      "pool2_conv : False\n",
      "pool2_pool : False\n",
      "conv3_block1_0_bn : False\n",
      "conv3_block1_0_relu : False\n",
      "conv3_block1_1_conv : False\n",
      "conv3_block1_1_bn : False\n",
      "conv3_block1_1_relu : False\n",
      "conv3_block1_2_conv : False\n",
      "conv3_block1_concat : False\n",
      "conv3_block2_0_bn : False\n",
      "conv3_block2_0_relu : False\n",
      "conv3_block2_1_conv : False\n",
      "conv3_block2_1_bn : False\n",
      "conv3_block2_1_relu : False\n",
      "conv3_block2_2_conv : False\n",
      "conv3_block2_concat : False\n",
      "conv3_block3_0_bn : False\n",
      "conv3_block3_0_relu : False\n",
      "conv3_block3_1_conv : False\n",
      "conv3_block3_1_bn : False\n",
      "conv3_block3_1_relu : False\n",
      "conv3_block3_2_conv : False\n",
      "conv3_block3_concat : False\n",
      "conv3_block4_0_bn : False\n",
      "conv3_block4_0_relu : False\n",
      "conv3_block4_1_conv : False\n",
      "conv3_block4_1_bn : False\n",
      "conv3_block4_1_relu : False\n",
      "conv3_block4_2_conv : False\n",
      "conv3_block4_concat : False\n",
      "conv3_block5_0_bn : False\n",
      "conv3_block5_0_relu : False\n",
      "conv3_block5_1_conv : False\n",
      "conv3_block5_1_bn : False\n",
      "conv3_block5_1_relu : False\n",
      "conv3_block5_2_conv : False\n",
      "conv3_block5_concat : False\n",
      "conv3_block6_0_bn : False\n",
      "conv3_block6_0_relu : False\n",
      "conv3_block6_1_conv : False\n",
      "conv3_block6_1_bn : False\n",
      "conv3_block6_1_relu : False\n",
      "conv3_block6_2_conv : False\n",
      "conv3_block6_concat : False\n",
      "conv3_block7_0_bn : False\n",
      "conv3_block7_0_relu : False\n",
      "conv3_block7_1_conv : False\n",
      "conv3_block7_1_bn : False\n",
      "conv3_block7_1_relu : False\n",
      "conv3_block7_2_conv : False\n",
      "conv3_block7_concat : False\n",
      "conv3_block8_0_bn : False\n",
      "conv3_block8_0_relu : False\n",
      "conv3_block8_1_conv : False\n",
      "conv3_block8_1_bn : False\n",
      "conv3_block8_1_relu : False\n",
      "conv3_block8_2_conv : False\n",
      "conv3_block8_concat : False\n",
      "conv3_block9_0_bn : False\n",
      "conv3_block9_0_relu : False\n",
      "conv3_block9_1_conv : False\n",
      "conv3_block9_1_bn : False\n",
      "conv3_block9_1_relu : False\n",
      "conv3_block9_2_conv : False\n",
      "conv3_block9_concat : False\n",
      "conv3_block10_0_bn : False\n",
      "conv3_block10_0_relu : False\n",
      "conv3_block10_1_conv : False\n",
      "conv3_block10_1_bn : False\n",
      "conv3_block10_1_relu : False\n",
      "conv3_block10_2_conv : False\n",
      "conv3_block10_concat : False\n",
      "conv3_block11_0_bn : False\n",
      "conv3_block11_0_relu : False\n",
      "conv3_block11_1_conv : False\n",
      "conv3_block11_1_bn : False\n",
      "conv3_block11_1_relu : False\n",
      "conv3_block11_2_conv : False\n",
      "conv3_block11_concat : False\n",
      "conv3_block12_0_bn : False\n",
      "conv3_block12_0_relu : False\n",
      "conv3_block12_1_conv : False\n",
      "conv3_block12_1_bn : False\n",
      "conv3_block12_1_relu : False\n",
      "conv3_block12_2_conv : False\n",
      "conv3_block12_concat : False\n",
      "pool3_bn : False\n",
      "pool3_relu : False\n",
      "pool3_conv : False\n",
      "pool3_pool : False\n",
      "conv4_block1_0_bn : False\n",
      "conv4_block1_0_relu : False\n",
      "conv4_block1_1_conv : False\n",
      "conv4_block1_1_bn : False\n",
      "conv4_block1_1_relu : False\n",
      "conv4_block1_2_conv : False\n",
      "conv4_block1_concat : False\n",
      "conv4_block2_0_bn : False\n",
      "conv4_block2_0_relu : False\n",
      "conv4_block2_1_conv : False\n",
      "conv4_block2_1_bn : False\n",
      "conv4_block2_1_relu : False\n",
      "conv4_block2_2_conv : False\n",
      "conv4_block2_concat : False\n",
      "conv4_block3_0_bn : False\n",
      "conv4_block3_0_relu : False\n",
      "conv4_block3_1_conv : False\n",
      "conv4_block3_1_bn : False\n",
      "conv4_block3_1_relu : False\n",
      "conv4_block3_2_conv : False\n",
      "conv4_block3_concat : False\n",
      "conv4_block4_0_bn : False\n",
      "conv4_block4_0_relu : False\n",
      "conv4_block4_1_conv : False\n",
      "conv4_block4_1_bn : False\n",
      "conv4_block4_1_relu : False\n",
      "conv4_block4_2_conv : False\n",
      "conv4_block4_concat : False\n",
      "conv4_block5_0_bn : False\n",
      "conv4_block5_0_relu : False\n",
      "conv4_block5_1_conv : False\n",
      "conv4_block5_1_bn : False\n",
      "conv4_block5_1_relu : False\n",
      "conv4_block5_2_conv : False\n",
      "conv4_block5_concat : False\n",
      "conv4_block6_0_bn : False\n",
      "conv4_block6_0_relu : False\n",
      "conv4_block6_1_conv : False\n",
      "conv4_block6_1_bn : False\n",
      "conv4_block6_1_relu : False\n",
      "conv4_block6_2_conv : False\n",
      "conv4_block6_concat : False\n",
      "conv4_block7_0_bn : False\n",
      "conv4_block7_0_relu : False\n",
      "conv4_block7_1_conv : False\n",
      "conv4_block7_1_bn : False\n",
      "conv4_block7_1_relu : False\n",
      "conv4_block7_2_conv : False\n",
      "conv4_block7_concat : False\n",
      "conv4_block8_0_bn : False\n",
      "conv4_block8_0_relu : False\n",
      "conv4_block8_1_conv : False\n",
      "conv4_block8_1_bn : False\n",
      "conv4_block8_1_relu : False\n",
      "conv4_block8_2_conv : False\n",
      "conv4_block8_concat : False\n",
      "conv4_block9_0_bn : False\n",
      "conv4_block9_0_relu : False\n",
      "conv4_block9_1_conv : False\n",
      "conv4_block9_1_bn : False\n",
      "conv4_block9_1_relu : False\n",
      "conv4_block9_2_conv : False\n",
      "conv4_block9_concat : False\n",
      "conv4_block10_0_bn : False\n",
      "conv4_block10_0_relu : False\n",
      "conv4_block10_1_conv : False\n",
      "conv4_block10_1_bn : False\n",
      "conv4_block10_1_relu : False\n",
      "conv4_block10_2_conv : False\n",
      "conv4_block10_concat : False\n",
      "conv4_block11_0_bn : False\n",
      "conv4_block11_0_relu : False\n",
      "conv4_block11_1_conv : False\n",
      "conv4_block11_1_bn : False\n",
      "conv4_block11_1_relu : False\n",
      "conv4_block11_2_conv : False\n",
      "conv4_block11_concat : False\n",
      "conv4_block12_0_bn : False\n",
      "conv4_block12_0_relu : False\n",
      "conv4_block12_1_conv : False\n",
      "conv4_block12_1_bn : False\n",
      "conv4_block12_1_relu : False\n",
      "conv4_block12_2_conv : False\n",
      "conv4_block12_concat : False\n",
      "conv4_block13_0_bn : False\n",
      "conv4_block13_0_relu : False\n",
      "conv4_block13_1_conv : False\n",
      "conv4_block13_1_bn : False\n",
      "conv4_block13_1_relu : False\n",
      "conv4_block13_2_conv : False\n",
      "conv4_block13_concat : False\n",
      "conv4_block14_0_bn : False\n",
      "conv4_block14_0_relu : False\n",
      "conv4_block14_1_conv : False\n",
      "conv4_block14_1_bn : False\n",
      "conv4_block14_1_relu : False\n",
      "conv4_block14_2_conv : False\n",
      "conv4_block14_concat : False\n",
      "conv4_block15_0_bn : False\n",
      "conv4_block15_0_relu : False\n",
      "conv4_block15_1_conv : False\n",
      "conv4_block15_1_bn : False\n",
      "conv4_block15_1_relu : False\n",
      "conv4_block15_2_conv : False\n",
      "conv4_block15_concat : False\n",
      "conv4_block16_0_bn : False\n",
      "conv4_block16_0_relu : False\n",
      "conv4_block16_1_conv : False\n",
      "conv4_block16_1_bn : False\n",
      "conv4_block16_1_relu : False\n",
      "conv4_block16_2_conv : False\n",
      "conv4_block16_concat : False\n",
      "conv4_block17_0_bn : False\n",
      "conv4_block17_0_relu : False\n",
      "conv4_block17_1_conv : False\n",
      "conv4_block17_1_bn : False\n",
      "conv4_block17_1_relu : False\n",
      "conv4_block17_2_conv : False\n",
      "conv4_block17_concat : False\n",
      "conv4_block18_0_bn : False\n",
      "conv4_block18_0_relu : False\n",
      "conv4_block18_1_conv : False\n",
      "conv4_block18_1_bn : False\n",
      "conv4_block18_1_relu : False\n",
      "conv4_block18_2_conv : False\n",
      "conv4_block18_concat : False\n",
      "conv4_block19_0_bn : False\n",
      "conv4_block19_0_relu : False\n",
      "conv4_block19_1_conv : False\n",
      "conv4_block19_1_bn : False\n",
      "conv4_block19_1_relu : False\n",
      "conv4_block19_2_conv : False\n",
      "conv4_block19_concat : False\n",
      "conv4_block20_0_bn : False\n",
      "conv4_block20_0_relu : False\n",
      "conv4_block20_1_conv : False\n",
      "conv4_block20_1_bn : False\n",
      "conv4_block20_1_relu : False\n",
      "conv4_block20_2_conv : False\n",
      "conv4_block20_concat : False\n",
      "conv4_block21_0_bn : False\n",
      "conv4_block21_0_relu : False\n",
      "conv4_block21_1_conv : False\n",
      "conv4_block21_1_bn : False\n",
      "conv4_block21_1_relu : False\n",
      "conv4_block21_2_conv : False\n",
      "conv4_block21_concat : False\n",
      "conv4_block22_0_bn : False\n",
      "conv4_block22_0_relu : False\n",
      "conv4_block22_1_conv : False\n",
      "conv4_block22_1_bn : False\n",
      "conv4_block22_1_relu : False\n",
      "conv4_block22_2_conv : False\n",
      "conv4_block22_concat : False\n",
      "conv4_block23_0_bn : False\n",
      "conv4_block23_0_relu : False\n",
      "conv4_block23_1_conv : False\n",
      "conv4_block23_1_bn : False\n",
      "conv4_block23_1_relu : False\n",
      "conv4_block23_2_conv : False\n",
      "conv4_block23_concat : False\n",
      "conv4_block24_0_bn : False\n",
      "conv4_block24_0_relu : False\n",
      "conv4_block24_1_conv : False\n",
      "conv4_block24_1_bn : False\n",
      "conv4_block24_1_relu : False\n",
      "conv4_block24_2_conv : False\n",
      "conv4_block24_concat : False\n",
      "pool4_bn : False\n",
      "pool4_relu : False\n",
      "pool4_conv : False\n",
      "pool4_pool : False\n",
      "conv5_block1_0_bn : False\n",
      "conv5_block1_0_relu : False\n",
      "conv5_block1_1_conv : False\n",
      "conv5_block1_1_bn : False\n",
      "conv5_block1_1_relu : False\n",
      "conv5_block1_2_conv : False\n",
      "conv5_block1_concat : False\n",
      "conv5_block2_0_bn : False\n",
      "conv5_block2_0_relu : False\n",
      "conv5_block2_1_conv : False\n",
      "conv5_block2_1_bn : False\n",
      "conv5_block2_1_relu : False\n",
      "conv5_block2_2_conv : False\n",
      "conv5_block2_concat : False\n",
      "conv5_block3_0_bn : False\n",
      "conv5_block3_0_relu : False\n",
      "conv5_block3_1_conv : False\n",
      "conv5_block3_1_bn : False\n",
      "conv5_block3_1_relu : False\n",
      "conv5_block3_2_conv : False\n",
      "conv5_block3_concat : False\n",
      "conv5_block4_0_bn : False\n",
      "conv5_block4_0_relu : False\n",
      "conv5_block4_1_conv : False\n",
      "conv5_block4_1_bn : False\n",
      "conv5_block4_1_relu : False\n",
      "conv5_block4_2_conv : False\n",
      "conv5_block4_concat : False\n",
      "conv5_block5_0_bn : False\n",
      "conv5_block5_0_relu : False\n",
      "conv5_block5_1_conv : False\n",
      "conv5_block5_1_bn : False\n",
      "conv5_block5_1_relu : False\n",
      "conv5_block5_2_conv : False\n",
      "conv5_block5_concat : False\n",
      "conv5_block6_0_bn : False\n",
      "conv5_block6_0_relu : False\n",
      "conv5_block6_1_conv : False\n",
      "conv5_block6_1_bn : False\n",
      "conv5_block6_1_relu : False\n",
      "conv5_block6_2_conv : False\n",
      "conv5_block6_concat : False\n",
      "conv5_block7_0_bn : False\n",
      "conv5_block7_0_relu : False\n",
      "conv5_block7_1_conv : False\n",
      "conv5_block7_1_bn : False\n",
      "conv5_block7_1_relu : False\n",
      "conv5_block7_2_conv : False\n",
      "conv5_block7_concat : False\n",
      "conv5_block8_0_bn : False\n",
      "conv5_block8_0_relu : False\n",
      "conv5_block8_1_conv : False\n",
      "conv5_block8_1_bn : False\n",
      "conv5_block8_1_relu : False\n",
      "conv5_block8_2_conv : False\n",
      "conv5_block8_concat : False\n",
      "conv5_block9_0_bn : False\n",
      "conv5_block9_0_relu : False\n",
      "conv5_block9_1_conv : False\n",
      "conv5_block9_1_bn : False\n",
      "conv5_block9_1_relu : False\n",
      "conv5_block9_2_conv : False\n",
      "conv5_block9_concat : False\n",
      "conv5_block10_0_bn : False\n",
      "conv5_block10_0_relu : False\n",
      "conv5_block10_1_conv : False\n",
      "conv5_block10_1_bn : False\n",
      "conv5_block10_1_relu : False\n",
      "conv5_block10_2_conv : False\n",
      "conv5_block10_concat : False\n",
      "conv5_block11_0_bn : False\n",
      "conv5_block11_0_relu : False\n",
      "conv5_block11_1_conv : False\n",
      "conv5_block11_1_bn : False\n",
      "conv5_block11_1_relu : False\n",
      "conv5_block11_2_conv : False\n",
      "conv5_block11_concat : False\n",
      "conv5_block12_0_bn : False\n",
      "conv5_block12_0_relu : False\n",
      "conv5_block12_1_conv : False\n",
      "conv5_block12_1_bn : False\n",
      "conv5_block12_1_relu : False\n",
      "conv5_block12_2_conv : False\n",
      "conv5_block12_concat : False\n",
      "conv5_block13_0_bn : False\n",
      "conv5_block13_0_relu : False\n",
      "conv5_block13_1_conv : False\n",
      "conv5_block13_1_bn : False\n",
      "conv5_block13_1_relu : False\n",
      "conv5_block13_2_conv : False\n",
      "conv5_block13_concat : False\n",
      "conv5_block14_0_bn : False\n",
      "conv5_block14_0_relu : False\n",
      "conv5_block14_1_conv : False\n",
      "conv5_block14_1_bn : False\n",
      "conv5_block14_1_relu : False\n",
      "conv5_block14_2_conv : False\n",
      "conv5_block14_concat : False\n",
      "conv5_block15_0_bn : False\n",
      "conv5_block15_0_relu : False\n",
      "conv5_block15_1_conv : False\n",
      "conv5_block15_1_bn : False\n",
      "conv5_block15_1_relu : False\n",
      "conv5_block15_2_conv : False\n",
      "conv5_block15_concat : False\n",
      "conv5_block16_0_bn : False\n",
      "conv5_block16_0_relu : False\n",
      "conv5_block16_1_conv : False\n",
      "conv5_block16_1_bn : False\n",
      "conv5_block16_1_relu : False\n",
      "conv5_block16_2_conv : False\n",
      "conv5_block16_concat : False\n",
      "bn : False\n",
      "relu : False\n",
      "global_average_pooling2d_1 : False\n",
      "dense_logits : True\n",
      "predictions : True\n"
     ]
    }
   ],
   "source": [
    "for layers in model.layers:\n",
    "    print(str(layers.name) + \" : \" + str(layers.trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 606\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 604\n"
     ]
    }
   ],
   "source": [
    "print(\"weights:\", len(model.weights))\n",
    "print(\"trainable_weights:\", len(model.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(model.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate is increased to 3e-3 instead of 1e-3\n",
    "# learning rate decreased by factor of 2 instead of 10\n",
    "# batch_size is increased to 512\n",
    "\n",
    "learning_rate = 3e-3\n",
    "momentum_val=0.9\n",
    "decay_val= 0.0\n",
    "batch_s = 512 # may need to reduce batch size if OOM error occurs\n",
    "train_batch_size = batch_s\n",
    "test_batch_size = 256\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=2, min_lr=1e-5, verbose=1)\n",
    "\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, decay=decay_val)\n",
    "adam_opt = tf.keras.mixed_precision.LossScaleOptimizer(adam_opt)\n",
    "\n",
    "model.compile(optimizer=adam_opt,\n",
    "                loss=tf.losses.CategoricalCrossentropy(),\n",
    "                metrics=[\n",
    "                    tf.keras.metrics.AUC(curve='ROC', name='ROC-AUC'),\n",
    "                    tf.keras.metrics.AUC(curve='PR', name='PR-AUC')\n",
    "                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#up sampling 'ASIAN' and 'BLACK/AFRICAN AMERICAN' classes\n",
    "\n",
    "train_df = data_df[data_df.split==\"train\"]\n",
    "other_df = train_df[train_df.race!=\"WHITE\"]\n",
    "train_df = pd.concat([other_df, train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHITE                     113334\n",
       "BLACK/AFRICAN AMERICAN     54306\n",
       "ASIAN                      10690\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 178330 validated image filenames belonging to 3 classes.\n",
      "Found 19065 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = train_gen.flow_from_dataframe(train_df, directory=\"/path/to/directory/\", x_col=\"path\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=True,seed=2021,batch_size=train_batch_size, dtype='float32')\n",
    "validate_batches = validate_gen.flow_from_dataframe(validation_df, directory=\"/path/to/directory/\", x_col=\"path\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=False,batch_size=test_batch_size, dtype='float32')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = math.ceil(len(train_df) / train_batch_size)\n",
    "val_epoch = math.ceil(len(validation_df) / test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_date = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ES = EarlyStopping(monitor='val_loss', mode='min', patience=6, restore_best_weights=True)\n",
    "checkloss = ModelCheckpoint(\"../saved_models/racial_bias/trials/\" + str(arc_name) + \"_CXR_LR-\" + str(learning_rate) + \"_\" + var_date+\"_epoch_{epoch:03d}_val_loss_{val_loss:.5f}.hdf5\", monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = nccl, num_packs = 1\n",
      "349/349 [==============================] - 401s 1s/step - loss: 0.6850 - ROC-AUC: 0.8724 - PR-AUC: 0.7724 - val_loss: 0.5279 - val_ROC-AUC: 0.9236 - val_PR-AUC: 0.8622\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52793, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.003_20211013-212652_epoch_001_val_loss_0.52793.hdf5\n",
      "Epoch 2/100\n",
      "349/349 [==============================] - 303s 842ms/step - loss: 0.6401 - ROC-AUC: 0.8890 - PR-AUC: 0.8025 - val_loss: 0.5320 - val_ROC-AUC: 0.9239 - val_PR-AUC: 0.8610\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.52793\n",
      "Epoch 3/100\n",
      "349/349 [==============================] - 304s 842ms/step - loss: 0.6228 - ROC-AUC: 0.8947 - PR-AUC: 0.8142 - val_loss: 0.5151 - val_ROC-AUC: 0.9283 - val_PR-AUC: 0.8737\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52793 to 0.51507, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.003_20211013-212652_epoch_003_val_loss_0.51507.hdf5\n",
      "Epoch 4/100\n",
      "349/349 [==============================] - 303s 841ms/step - loss: 0.6161 - ROC-AUC: 0.8970 - PR-AUC: 0.8178 - val_loss: 0.5152 - val_ROC-AUC: 0.9284 - val_PR-AUC: 0.8723\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51507\n",
      "Epoch 5/100\n",
      "349/349 [==============================] - 302s 839ms/step - loss: 0.6123 - ROC-AUC: 0.8983 - PR-AUC: 0.8193 - val_loss: 0.5278 - val_ROC-AUC: 0.9254 - val_PR-AUC: 0.8695\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51507\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "Epoch 6/100\n",
      "349/349 [==============================] - 303s 839ms/step - loss: 0.6031 - ROC-AUC: 0.9012 - PR-AUC: 0.8255 - val_loss: 0.5033 - val_ROC-AUC: 0.9315 - val_PR-AUC: 0.8783\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51507 to 0.50329, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.003_20211013-212652_epoch_006_val_loss_0.50329.hdf5\n",
      "Epoch 7/100\n",
      "349/349 [==============================] - 303s 841ms/step - loss: 0.6016 - ROC-AUC: 0.9017 - PR-AUC: 0.8266 - val_loss: 0.5158 - val_ROC-AUC: 0.9285 - val_PR-AUC: 0.8761\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.50329\n",
      "Epoch 8/100\n",
      "349/349 [==============================] - 303s 841ms/step - loss: 0.5998 - ROC-AUC: 0.9023 - PR-AUC: 0.8277 - val_loss: 0.4981 - val_ROC-AUC: 0.9332 - val_PR-AUC: 0.8818\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.50329 to 0.49810, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.003_20211013-212652_epoch_008_val_loss_0.49810.hdf5\n",
      "Epoch 9/100\n",
      "349/349 [==============================] - 304s 843ms/step - loss: 0.6003 - ROC-AUC: 0.9021 - PR-AUC: 0.8273 - val_loss: 0.4957 - val_ROC-AUC: 0.9340 - val_PR-AUC: 0.8832\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.49810 to 0.49566, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.003_20211013-212652_epoch_009_val_loss_0.49566.hdf5\n",
      "Epoch 10/100\n",
      "349/349 [==============================] - 304s 842ms/step - loss: 0.5966 - ROC-AUC: 0.9034 - PR-AUC: 0.8295 - val_loss: 0.4965 - val_ROC-AUC: 0.9333 - val_PR-AUC: 0.8814\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49566\n",
      "Epoch 11/100\n",
      "349/349 [==============================] - 302s 837ms/step - loss: 0.5956 - ROC-AUC: 0.9037 - PR-AUC: 0.8299 - val_loss: 0.5097 - val_ROC-AUC: 0.9302 - val_PR-AUC: 0.8778\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.49566\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "Epoch 12/100\n",
      "349/349 [==============================] - 303s 842ms/step - loss: 0.5929 - ROC-AUC: 0.9045 - PR-AUC: 0.8316 - val_loss: 0.4933 - val_ROC-AUC: 0.9342 - val_PR-AUC: 0.8832\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.49566 to 0.49328, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.003_20211013-212652_epoch_012_val_loss_0.49328.hdf5\n",
      "Epoch 13/100\n",
      "349/349 [==============================] - 304s 844ms/step - loss: 0.5923 - ROC-AUC: 0.9048 - PR-AUC: 0.8320 - val_loss: 0.4935 - val_ROC-AUC: 0.9345 - val_PR-AUC: 0.8840\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49328\n",
      "Epoch 14/100\n",
      "349/349 [==============================] - 303s 840ms/step - loss: 0.5913 - ROC-AUC: 0.9051 - PR-AUC: 0.8327 - val_loss: 0.4935 - val_ROC-AUC: 0.9341 - val_PR-AUC: 0.8834\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49328\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "Epoch 15/100\n",
      "349/349 [==============================] - 302s 838ms/step - loss: 0.5906 - ROC-AUC: 0.9053 - PR-AUC: 0.8331 - val_loss: 0.4940 - val_ROC-AUC: 0.9339 - val_PR-AUC: 0.8828\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49328\n",
      "Epoch 16/100\n",
      "349/349 [==============================] - 304s 843ms/step - loss: 0.5894 - ROC-AUC: 0.9056 - PR-AUC: 0.8336 - val_loss: 0.4894 - val_ROC-AUC: 0.9351 - val_PR-AUC: 0.8849\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.49328 to 0.48939, saving model to ../saved_models/racial_bias/trials/MIMIC-320x320_80-10-10-split-DenseNet121-Float16_pathology_detection__CXR_LR-0.003_20211013-212652_epoch_016_val_loss_0.48939.hdf5\n",
      "Epoch 17/100\n",
      "349/349 [==============================] - 303s 842ms/step - loss: 0.5885 - ROC-AUC: 0.9060 - PR-AUC: 0.8344 - val_loss: 0.4928 - val_ROC-AUC: 0.9344 - val_PR-AUC: 0.8838\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.48939\n",
      "Epoch 18/100\n",
      "349/349 [==============================] - 302s 838ms/step - loss: 0.5884 - ROC-AUC: 0.9060 - PR-AUC: 0.8340 - val_loss: 0.4948 - val_ROC-AUC: 0.9338 - val_PR-AUC: 0.8826\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48939\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "Epoch 19/100\n",
      "349/349 [==============================] - 304s 843ms/step - loss: 0.5876 - ROC-AUC: 0.9063 - PR-AUC: 0.8347 - val_loss: 0.4917 - val_ROC-AUC: 0.9346 - val_PR-AUC: 0.8838\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.48939\n",
      "Epoch 20/100\n",
      "349/349 [==============================] - 304s 844ms/step - loss: 0.5877 - ROC-AUC: 0.9062 - PR-AUC: 0.8345 - val_loss: 0.4929 - val_ROC-AUC: 0.9343 - val_PR-AUC: 0.8837\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.48939\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "Epoch 21/100\n",
      "349/349 [==============================] - 304s 843ms/step - loss: 0.5873 - ROC-AUC: 0.9064 - PR-AUC: 0.8348 - val_loss: 0.4909 - val_ROC-AUC: 0.9347 - val_PR-AUC: 0.8842\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48939\n",
      "Epoch 22/100\n",
      "349/349 [==============================] - 302s 838ms/step - loss: 0.5866 - ROC-AUC: 0.9066 - PR-AUC: 0.8353 - val_loss: 0.4928 - val_ROC-AUC: 0.9344 - val_PR-AUC: 0.8839\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48939\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5a5d0ccba8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_batches,\n",
    "            validation_data=validate_batches,\n",
    "            epochs=100,\n",
    "            steps_per_epoch=int(train_epoch),\n",
    "            validation_steps=int(val_epoch),\n",
    "            workers=32,\n",
    "            max_queue_size=50,\n",
    "            shuffle=True,\n",
    "            callbacks=[checkloss, reduce_lr, ES]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18320 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = validate_gen.flow_from_dataframe(test_df, directory=\"/tf/notebooks/SSD_data/mimic_directory/resize_mimic_320x320/\", x_col=\"path\", y_col=\"race\", class_mode=\"categorical\",target_size=(HEIGHT, WIDTH),shuffle=False,batch_size=test_batch_size, dtype='float32')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 34s 311ms/step\n"
     ]
    }
   ],
   "source": [
    "multilabel_predict_test = model.predict(test_batches, max_queue_size=10, verbose=1, steps=math.ceil(len(test_df)/test_batch_size), workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classwise ROC AUC \n",
      "\n",
      "Class - Asian ROC-AUC- 0.7986\n",
      "Class - Black ROC-AUC- 0.8416\n",
      "Class - White ROC-AUC- 0.8258\n"
     ]
    }
   ],
   "source": [
    "result = multilabel_predict_test\n",
    "labels = np.argmax(result, axis=1)\n",
    "target_names = ['Asian', 'Black', 'White']\n",
    "\n",
    "print ('Classwise ROC AUC \\n')\n",
    "for p in list(set(labels)):\n",
    "    fpr, tpr, thresholds = roc_curve(test_batches.classes, result[:,p], pos_label = p)\n",
    "    auroc = round(auc(fpr, tpr), 4)\n",
    "    print ('Class - {} ROC-AUC- {}'.format(target_names[p], auroc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
